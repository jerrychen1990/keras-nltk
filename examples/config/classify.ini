[common]
# 模型日志输出的级别
log_level = DEBUG
# 是否训练模型
is_train = True
# 是否要测评模型并输出模型测评结果
is_eval = True
# 是否要输出模型预测结果
is_output = True
# 是否要保存模型
is_saving = True

[data]
# 训练数据地址，json格式
train_data_path = data/classify.json
# 验证数据地址，json格式
dev_data_path = data/classify.json
# 测试数据地址，json格式
test_data_path = data/classify.json

[schema]
# 词表地址
vocab_path = schema/vocab.txt
# label标签字典
label_dict_path = schema/label_dict.json


[model]
# ner_extractor的checkpoint路径，输入路径的话从上次的checkpoint继续训练，留空的话从头开始训练
ckp_path =
# 模型接受的最大输入长度(token的长度)
max_len = 100
# 模型名称(会用在模型保存、评价结果保存、tensorboard日志命名)
model_name = classify-model-v1

[model_args]
# 是否使用bert
use_bert = False
# 是否使用bi-lstm
use_lstm = False
# 是否fine-tune bert
fine_tune_bert = True
# 读入的bert模型路径
bert_ckpt_path = ../../pretrained_model/chinese_L-12_H-768_A-12
# 如果使用自己fine-tune的keras bert模型，bert_keras_path为keras的.h5模型文件路径
bert_keras_path =
# 如果不使用bert，word_embedding_dim为自己训练的token向量维度
#word_embedding_dim = 4
# lstm中隐向量的维度
#lstm_dim = 8
# fine-tune bert 过程中，要fix住的bert层数（从最低层开始计数）
#freeze_layer_num=0
# 最终softmax分类之前的全连接层的纬度数，默认用relu激活函数
#dense_dim_list=[]
# softmax层的dropout比率
#drop_out_rate=0.2
# l2正则化参数
#l2=0.01
# l1正则化参数
#l1=0.01

[compile_args]
# 训练时使用的gpu数目
gpu_num = 1
# 优化器名称,可选值详见 eigen_nltk.optimizer.OPTIMIZER_DICT
optimizer_name=bert_adam
# 优化器的参数，字典格式
optimizer_args=dict(lr=3e-5)

[train_args]
# batch大小
batch_size = 4
# 一个epoch 执行多少个batch
steps_per_epoch = 4
# epoch数
epochs = 10

[callback_args]
patience = 5
monitor = val_loss


