[common]
# 模型日志输出的级别
log_level = DEBUG
# 是否训练模型
is_train = True
# 是否要测评模型并输出模型测评结果，pretrain模型暂时不支持eval
is_eval = False
# 是否要输出模型预测结果， pretrain模型暂时不支持output
is_output = False
# 是否要保存模型
is_saving = True

[data]
# 训练数据地址，json格式
train_data_path = data/pretrain.json
# 验证数据地址，json格式
dev_data_path = data/pretrain.json
# 测试数据地址，json格式
test_data_path = data/pretrain.json

[schema]
# 词表地址
vocab_path = schema/vocab-small.txt

[model]
# ner_extractor的checkpoint路径，输入路径的话从上次的checkpoint继续训练，留空的话从头开始训练
ckp_path =
# 模型接受的最大输入长度(token的长度)
max_len = 150
# 模型名称(会用在模型保存、评价结果保存、tensorboard日志命名)
model_name = bert-pretrain-v1

[model_args]
# 读入的bert模型路径
bert_ckpt_path =
# 如果使用自己fine-tune的keras bert模型，bert_keras_path为keras的.h5模型文件路径
bert_keras_path =
# 如果bert_ckpt_path和bert_keras_path都为空，会新建一个小的bert模型，用于测试

[train_args]
# batch大小
batch_size = 4
# 一个epoch 执行多少个batch
steps_per_epoch = 4
# epoch数
epochs = 10


[compile_args]
# 训练时使用的gpu数目
gpu_num = 1
# 优化器名称,可选值详见 eigen_nltk.optimizer.OPTIMIZER_DICT
optimizer_name=bert_adam
# 优化器的参数，字典格式
optimizer_args=dict(lr=3e-5)

[callback_args]
patience = 5
monitor = val_loss


